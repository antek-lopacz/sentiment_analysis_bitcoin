{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Core / scientific\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as nn_utils\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# scikit-learn (+ Intel acceleration)\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Gensim / NLTK\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Serialization\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "# Warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "# Local packages\n",
    "sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
    "from functions import textprep\n",
    "from functions import evalkit\n",
    "from project_paths import DATA_DIR, FILES_DIR, TSB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce GTX 970\n",
      "CUDA Version: 12.4\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# PyTorch info\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Word2Vec embeddings\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "# Load the data\n",
    "reddit_data = pd.read_csv(DATA_DIR / \"reddit_sentiment_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict_bert_sentiment = {0: \"Very negative\",\n",
    "                             1: \"Negative\",\n",
    "                             2: \"Neutral\",\n",
    "                             3: \"Positive\",\n",
    "                             4: \"Very positive\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Tokenization and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to keep (won't be removed as stopwords)\n",
    "custom_words_to_keep = [\n",
    "    'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", \n",
    "    'doesn', \"doesn't\", 'don', \"don't\", 'hadn', \"hadn't\", \n",
    "    'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "    'mustn', \"mustn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
    "    \"won't\", 'wouldn', \"wouldn't\",\n",
    "    'above', 'below', 'down', 'up'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache paths in DATA_DIR\n",
    "preproc_path = FILES_DIR / \"pkl_files\" / \"clean_tokens_and_body.pkl\"\n",
    "emb_path = FILES_DIR / \"pkl_files\" / \"sentence_embedding.pkl\"\n",
    "\n",
    "tqdm.pandas()\n",
    "def preprocess_tokens(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if pd.isna(text) else str(text)\n",
    "    toks = textprep.tokenize_function(text)\n",
    "    toks = textprep.clean_tokens(toks, custom_words_to_keep=custom_words_to_keep)\n",
    "    toks = textprep.replace_numerical_tokens(toks)\n",
    "    return toks\n",
    "\n",
    "# load or compute clean_tokens / clean_body\n",
    "if preproc_path.exists():\n",
    "    tmp = pd.read_pickle(preproc_path)\n",
    "    reddit_data[\"clean_tokens\"] = tmp.loc[reddit_data.index, \"clean_tokens\"]\n",
    "    reddit_data[\"clean_body\"] = tmp.loc[reddit_data.index, \"clean_body\"]\n",
    "else:\n",
    "    reddit_data[\"clean_tokens\"] = reddit_data[\"body\"].fillna(\"\").progress_map(preprocess_tokens)\n",
    "    reddit_data[\"clean_body\"] = reddit_data[\"body\"].fillna(\"\").progress_map(textprep.clean_body)\n",
    "    pd.DataFrame({\"clean_tokens\": reddit_data[\"clean_tokens\"], \"clean_body\": reddit_data[\"clean_body\"]}).to_pickle(preproc_path)\n",
    "\n",
    "# load or compute sentence_embedding (single post embedding by averaging word embeddings)\n",
    "if emb_path.exists():\n",
    "    reddit_data[\"sentence_embedding\"] = pd.read_pickle(emb_path).reindex(reddit_data.index)\n",
    "else:\n",
    "    reddit_data[\"sentence_embedding\"] = reddit_data[\"clean_tokens\"].apply(lambda tokens: textprep.post_to_embedding(tokens, word2vec))\n",
    "    reddit_data[\"sentence_embedding\"].to_pickle(emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Train and Test sets preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: 60% train, 20% validation, 20% test \n",
    "train_data, temp_data = train_test_split(reddit_data, test_size=0.4, random_state=42)  \n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack embeddings into a single NumPy array\n",
    "sentence_embeddings_train = np.array(train_data['sentence_embedding'].tolist())  \n",
    "sentence_embeddings_val = np.array(val_data['sentence_embedding'].tolist()) \n",
    "sentence_embeddings_test = np.array(test_data['sentence_embedding'].tolist())  \n",
    "\n",
    "# Convert sentence embeddings and labels to tensors\n",
    "X_train = torch.tensor(sentence_embeddings_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(sentence_embeddings_val, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(sentence_embeddings_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "labels_train = np.array(train_data['sentiment_label'].tolist())\n",
    "labels_val = np.array(val_data['sentiment_label'].tolist())\n",
    "labels_test = np.array(test_data['sentiment_label'].tolist())\n",
    "\n",
    "y_train = torch.tensor(labels_train, dtype=torch.long).to(device)\n",
    "y_val = torch.tensor(labels_val, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(labels_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text for BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def _load_or_tokenize(series, path):\n",
    "    if path.exists():\n",
    "        return torch.load(path, map_location='cpu', weights_only=False)\n",
    "    enc = textprep.bert_tokenize_function(series.fillna('').tolist(), bert_tokenizer)\n",
    "    torch.save(enc, path)\n",
    "    return enc\n",
    "\n",
    "train_path = FILES_DIR / \"pt_files\" / \"X_bert_train.pt\"\n",
    "val_path = FILES_DIR / \"pt_files\" / \"X_bert_val.pt\"\n",
    "test_path = FILES_DIR / \"pt_files\" / \"X_bert_test.pt\"\n",
    "\n",
    "X_bert_train = _load_or_tokenize(train_data[\"clean_body\"], train_path)\n",
    "X_bert_val = _load_or_tokenize(val_data[\"clean_body\"], val_path)\n",
    "X_bert_test = _load_or_tokenize(test_data[\"clean_body\"], test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or compute BERT embeddings and save to .npy files\n",
    "embedding_files = {\n",
    "    \"train\": \"X_bert_embeddings_train.npy\",\n",
    "    \"val\": \"X_bert_embeddings_val.npy\",\n",
    "    \"test\": \"X_bert_embeddings_test.npy\"\n",
    "}\n",
    "\n",
    "# Resolve target paths\n",
    "paths = {k: (Path(v) if Path(v).is_absolute() else (FILES_DIR / \"npy_files\" / v)) for k, v in embedding_files.items()}\n",
    "paths[\"train\"].parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "embeddings = {}\n",
    "missing = [k for k, p in paths.items() if not p.exists()]\n",
    "\n",
    "if missing:\n",
    "    from transformers import BertModel\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "    bert_model.eval()\n",
    "\n",
    "    datasets = {\"train\": X_bert_train, \"val\": X_bert_val, \"test\": X_bert_test}\n",
    "    for key, dataset in datasets.items():\n",
    "        if paths[key].exists():\n",
    "            embeddings[key] = np.load(paths[key], allow_pickle=False)\n",
    "        else:\n",
    "            embeddings[key] = np.array([\n",
    "                textprep.get_bert_embedding({\n",
    "                    \"input_ids\": dataset[\"input_ids\"][i].unsqueeze(0),\n",
    "                    \"attention_mask\": dataset[\"attention_mask\"][i].unsqueeze(0),\n",
    "                    device: device\n",
    "                }) for i in range(len(dataset[\"input_ids\"]))\n",
    "            ])\n",
    "            np.save(paths[key], embeddings[key])\n",
    "else:\n",
    "    embeddings = {k: np.load(p, allow_pickle=False) for k, p in paths.items()}\n",
    "\n",
    "X_bert_post_embeddings_train, X_bert_post_embeddings_val, X_bert_post_embeddings_test = embeddings[\"train\"], embeddings[\"val\"], embeddings[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets for Word2Vec based models\n",
    "train_dataset = TensorDataset(X_train.to(device), y_train.to(device))\n",
    "val_dataset = TensorDataset(X_val.to(device), y_val.to(device))\n",
    "test_dataset = TensorDataset(X_test.to(device), y_test.to(device))\n",
    "\n",
    "# Create DataLoaders for Word2Vec based models\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets for BERT based models\n",
    "train_dataset_bert = TensorDataset(\n",
    "    X_bert_train[\"input_ids\"].to(device), X_bert_train[\"attention_mask\"].to(device), y_train.to(device))\n",
    "val_dataset_bert = TensorDataset(\n",
    "    X_bert_val[\"input_ids\"].to(device), X_bert_val[\"attention_mask\"].to(device), y_val.to(device))\n",
    "test_dataset_bert = TensorDataset(\n",
    "    X_bert_test[\"input_ids\"].to(device), X_bert_test[\"attention_mask\"].to(device), y_test.to(device))\n",
    "\n",
    "# Create DataLoaders for BERT based models\n",
    "batch_size = 32\n",
    "train_dataloader_bert = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader_bert = DataLoader(val_dataset_bert, batch_size=batch_size)\n",
    "test_dataloader_bert = DataLoader(test_dataset_bert, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Word2Vec + MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model naming convention: \n",
    "`<EmbeddingType>_<ArchitectureType>_<LayerConfig>_<Activation>_<Extras>_<Task>`\n",
    "\n",
    "#### Example Components\n",
    "\n",
    "- **Embedding Type**: Specify the input representation.  \n",
    "  Examples: `W2V` for Word2Vec, `BERT`, `GloVe`, `TF-IDF`, etc.\n",
    "\n",
    "- **Architecture Type**: Include the model type.  \n",
    "  Examples: `MLP`, `LSTM`, `GRU`, `TF` (Transformer), etc.\n",
    "\n",
    "- **Layer Configuration**: Use layer sizes or count.  \n",
    "  Examples: `128-64-32` for layer sizes or `3L` for 3 layers.\n",
    "\n",
    "- **Activation Function**: Specify the activation function.  \n",
    "  Examples: `ReLU`, `LeakyReLU`, `Tanh`, etc.\n",
    "\n",
    "- **Extras**: Include regularization, dropout, or batch normalization (if relevant).  \n",
    "  Examples: `DO30` for 30% dropout, `BN` for BatchNorm.\n",
    "\n",
    "- **Task**: Add a suffix to describe the task.  \n",
    "  Examples: `MC` for multi-class classification, `SC` for single-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.a. Multi-class classification\n",
    "- target variable: `BERT-Sentiment`\n",
    "- input: vector dim (300,1); sentence vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_MLP_1L_ReLU_MC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(W2V_MLP_1L_ReLU_MC, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 5).to(device)      # Single-layer network (input to output)\n",
    "        self.relu = nn.ReLU().to(device)             # ReLU activation\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)  # Softmax for multi-class output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.fc1(x)             # Direct pass to output layer\n",
    "        x = self.softmax(x)         # Softmax activation for probabilities\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_MLP_3L_ReLU_MC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(W2V_MLP_3L_ReLU_MC, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 128).to(device)  # Input layer to hidden layer 1\n",
    "        self.fc2 = nn.Linear(128, 64).to(device)   # Hidden layer 1 to hidden layer 2\n",
    "        self.fc3 = nn.Linear(64, 5).to(device)     # Hidden layer 2 to output layer (5 classes)\n",
    "        self.relu = nn.ReLU().to(device)           # ReLU activation\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)  # Softmax activation for multi-class output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.relu(self.fc1(x))  # Layer 1 with ReLU activation\n",
    "        x = self.relu(self.fc2(x))  # Layer 2 with ReLU activation\n",
    "        x = self.fc3(x)             # Output layer (logits)\n",
    "        x = self.softmax(x)         # Softmax activation for probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_MLP_3L_ReLU_MC_DO30(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(W2V_MLP_3L_ReLU_MC_DO30, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 128).to(device)  # Input layer to hidden layer 1\n",
    "        self.dropout1 = nn.Dropout(0.3).to(device) # Dropout after layer 1 (30%)\n",
    "        self.fc2 = nn.Linear(128, 64).to(device)   # Hidden layer 1 to hidden layer 2\n",
    "        self.dropout2 = nn.Dropout(0.3).to(device) # Dropout after layer 2 (30%)\n",
    "        self.fc3 = nn.Linear(64, 5).to(device)     # Hidden layer 2 to output layer (5 classes)\n",
    "        self.relu = nn.ReLU().to(device)           # ReLU activation\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)  # Softmax activation for multi-class output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.relu(self.fc1(x))              # Layer 1 with ReLU\n",
    "        x = self.dropout1(x)                    # Dropout after layer 1\n",
    "        x = self.relu(self.fc2(x))              # Layer 2 with ReLU\n",
    "        x = self.dropout2(x)                    # Dropout after layer 2\n",
    "        x = self.fc3(x)                         # Output layer (logits)\n",
    "        x = self.softmax(x)                     # Softmax activation for probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_MLP_5L_ReLU_MC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(W2V_MLP_5L_ReLU_MC, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 128).to(device)  # Input layer to hidden layer 1\n",
    "        self.fc2 = nn.Linear(128, 64).to(device)   # Hidden layer 1 to hidden layer 2\n",
    "        self.fc3 = nn.Linear(64, 32).to(device)    # Hidden layer 2 to hidden layer 3\n",
    "        self.fc4 = nn.Linear(32, 16).to(device)    # Hidden layer 3 to hidden layer 4\n",
    "        self.fc5 = nn.Linear(16, 5).to(device)     # Hidden layer 4 to output layer (5 classes)\n",
    "        self.relu = nn.ReLU().to(device)           # ReLU activation\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)  # Softmax activation for multi-class output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.relu(self.fc1(x))  # Pass through first layer with ReLU\n",
    "        x = self.relu(self.fc2(x))  # Pass through second layer with ReLU\n",
    "        x = self.relu(self.fc3(x))  # Pass through third layer with ReLU\n",
    "        x = self.relu(self.fc4(x))  # Pass through fourth layer with ReLU\n",
    "        x = self.fc5(x)             # Output layer (logits)\n",
    "        x = self.softmax(x)         # Softmax activation for probabilities\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_MLP_5L_ReLU_MC_BN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(W2V_MLP_5L_ReLU_MC_BN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 128).to(device)  # Input layer to hidden layer 1\n",
    "        self.fc2 = nn.Linear(128, 64).to(device)   # Hidden layer 1 to hidden layer 2\n",
    "        self.bn2 = nn.BatchNorm1d(64).to(device)   # BatchNorm after layer 2\n",
    "        self.fc3 = nn.Linear(64, 32).to(device)    # Hidden layer 2 to hidden layer 3\n",
    "        self.fc4 = nn.Linear(32, 16).to(device)    # Hidden layer 3 to hidden layer 4\n",
    "        self.bn4 = nn.BatchNorm1d(16).to(device)   # BatchNorm after layer 4\n",
    "        self.fc5 = nn.Linear(16, 5).to(device)     # Hidden layer 4 to output layer (5 classes)\n",
    "        self.relu = nn.ReLU().to(device)           # ReLU activation\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)  # Softmax activation for multi-class output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.relu(self.fc1(x))                 # Layer 1 with ReLU\n",
    "        x = self.relu(self.bn2(self.fc2(x)))       # Layer 2 with BatchNorm and ReLU\n",
    "        x = self.relu(self.fc3(x))                 # Layer 3 with ReLU\n",
    "        x = self.relu(self.bn4(self.fc4(x)))       # Layer 4 with BatchNorm and ReLU\n",
    "        x = self.fc5(x)                            # Output layer (logits)\n",
    "        x = self.softmax(x)                        # Softmax activation for probabilities\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_MLP_5L_ReLU_MC_BN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(W2V_MLP_5L_ReLU_MC_BN4, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 128).to(device)  # Input layer to hidden layer 1\n",
    "        self.bn1 = nn.BatchNorm1d(128).to(device)  # BatchNorm after layer 1\n",
    "        self.fc2 = nn.Linear(128, 64).to(device)   # Hidden layer 1 to hidden layer 2\n",
    "        self.bn2 = nn.BatchNorm1d(64).to(device)   # BatchNorm after layer 2\n",
    "        self.fc3 = nn.Linear(64, 32).to(device)    # Hidden layer 2 to hidden layer 3\n",
    "        self.bn3 = nn.BatchNorm1d(32).to(device)   # BatchNorm after layer 3\n",
    "        self.fc4 = nn.Linear(32, 16).to(device)    # Hidden layer 3 to hidden layer 4\n",
    "        self.bn4 = nn.BatchNorm1d(16).to(device)   # BatchNorm after layer 4\n",
    "        self.fc5 = nn.Linear(16, 5).to(device)     # Hidden layer 4 to output layer (5 classes)\n",
    "        self.relu = nn.ReLU().to(device)           # ReLU activation\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)  # Softmax activation for multi-class output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.relu(self.bn1(self.fc1(x)))  # Layer 1 with BatchNorm and ReLU\n",
    "        x = self.relu(self.bn2(self.fc2(x)))  # Layer 2 with BatchNorm and ReLU\n",
    "        x = self.relu(self.bn3(self.fc3(x)))  # Layer 3 with BatchNorm and ReLU\n",
    "        x = self.relu(self.bn4(self.fc4(x)))  # Layer 4 with BatchNorm and ReLU\n",
    "        x = self.fc5(x)                       # Output layer (logits)\n",
    "        x = self.softmax(x)                   # Softmax activation for probabilities\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.b. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training loop for W2V based models '''\n",
    "\n",
    "# Models to train\n",
    "models = [\n",
    "    # W2V_MLP_1L_ReLU_MC().to(device),\n",
    "    # W2V_MLP_3L_ReLU_MC().to(device),\n",
    "    # W2V_MLP_3L_ReLU_MC_DO30().to(device),\n",
    "    # W2V_MLP_5L_ReLU_MC().to(device),\n",
    "    # W2V_MLP_5L_ReLU_MC_BN2().to(device),\n",
    "    # W2V_MLP_5L_ReLU_MC_BN4().to(device)\n",
    "]\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def get_optimizer(model):\n",
    "    return optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "def validate_model(model, val_dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return val_loss / len(val_dataloader), correct / total\n",
    "\n",
    "num_epochs = 20\n",
    "patience = 3\n",
    "\n",
    "# Directory setup\n",
    "(FILES_DIR / \"pth_files\").mkdir(parents=True, exist_ok=True)\n",
    "(TSB_DIR / \"W2V\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=str(TSB_DIR / \"W2V\"))\n",
    "\n",
    "for model in models:\n",
    "    model = model.to(device)\n",
    "    optimizer = get_optimizer(model)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels_tensor in train_dataloader:\n",
    "            inputs, labels_tensor = inputs.to(device), labels_tensor.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels_tensor)\n",
    "            loss.backward()\n",
    "            nn_utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_dataloader)\n",
    "        \n",
    "        val_loss, val_accuracy = validate_model(model, val_dataloader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        writer.add_scalar(f'{model.__class__.__name__}/Train_Loss', avg_train_loss, epoch)\n",
    "        writer.add_scalar(f'{model.__class__.__name__}/Val_Loss', val_loss, epoch)\n",
    "        writer.add_scalar(f'{model.__class__.__name__}/Val_Accuracy', val_accuracy, epoch)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f} Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_count = 0\n",
    "            torch.save(model.state_dict(), FILES_DIR / \"pth_files\" / f\"{model.__class__.__name__}_best_weights.pth\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Finished training {model.__class__.__name__}.\\n\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.c. Testing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CHOOSE MODEL TO TEST '''\n",
    "models = [\n",
    "    W2V_MLP_1L_ReLU_MC().to(device),\n",
    "    W2V_MLP_3L_ReLU_MC().to(device),\n",
    "    # W2V_MLP_3L_ReLU_MC_DO30().to(device),\n",
    "    # W2V_MLP_5L_ReLU_MC().to(device),\n",
    "    # W2V_MLP_5L_ReLU_MC_BN2().to(device),\n",
    "    # W2V_MLP_5L_ReLU_MC_BN4().to(device)\n",
    "]\n",
    "\n",
    "# Load the weights\n",
    "for model in models:\n",
    "    _best = FILES_DIR / \"pth_files\" / f\"{model.__class__.__name__}_best_weights.pth\"\n",
    "    _std = FILES_DIR / \"pth_files\" / f\"{model.__class__.__name__}_weights.pth\"\n",
    "    _path = _best if _best.exists() else _std\n",
    "    model.load_state_dict(torch.load(_path, map_location=device, weights_only=False))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- W2V_MLP_1L_ReLU_MC ---\n",
      "Validation set:  Accuracy: 0.6737  F1(macro): 0.2925  F1(weighted): 0.6169\n",
      "Test set:  Accuracy: 0.6798  F1(macro): 0.2951  F1(weighted): 0.6226\n",
      "\n",
      "--- W2V_MLP_3L_ReLU_MC ---\n",
      "Validation set:  Accuracy: 0.6998  F1(macro): 0.3037  F1(weighted): 0.6404\n",
      "Test set:  Accuracy: 0.7048  F1(macro): 0.3058  F1(weighted): 0.6452\n"
     ]
    }
   ],
   "source": [
    "''' TESTING ON VALIDATION AND TEST SETS '''\n",
    "for model in models:\n",
    "    print(f\"\\n--- {model.__class__.__name__} ---\")\n",
    "    evalkit.evaluate(model, X_val, y_val, dataset_name=\"Validation\", device=device)\n",
    "    evalkit.evaluate(model, X_test, y_test, dataset_name=\"Test\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- W2V_MLP_1L_ReLU_MC ---\n",
      "Predicted Sentiment: Very negative\n",
      "\n",
      "--- W2V_MLP_3L_ReLU_MC ---\n",
      "Predicted Sentiment: Very negative\n"
     ]
    }
   ],
   "source": [
    "''' TESTING ON INPUT SENTENCE'''\n",
    "test_sentence = \"This is very negative sentence, omg, I hate it\"\n",
    "# test_sentence = \"The price is going down, you will lose all your money\"\n",
    "# test_sentence = \"Bitcoin is going up guys, we will be rich, nice\"\n",
    "# test_sentence = \"omg bro this can't be real, we lost, price is very low\"\n",
    "\n",
    "test_sentence_token_words = preprocess_tokens(test_sentence)\n",
    "test_sentence_embedding = textprep.post_to_embedding(test_sentence_token_words, word2vec)\n",
    "test_sentence_tensor = torch.tensor(test_sentence_embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n--- {model.__class__.__name__} ---\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(test_sentence_tensor)\n",
    "        pred = torch.argmax(torch.softmax(out, dim=1), dim=1).item()\n",
    "    sentiment = class_dict_bert_sentiment[pred]\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Word2Vec + SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.a. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svc__C': 10, 'svc__gamma': 0.001, 'svc__kernel': 'rbf'} 0.5342595557194253\n"
     ]
    }
   ],
   "source": [
    "pkl_dir = FILES_DIR / \"pkl_files\"\n",
    "pkl_dir.mkdir(parents=True, exist_ok=True)\n",
    "grid_path = pkl_dir / \"svm_W2V_grid_search.pkl\"\n",
    "\n",
    "# Find the best hyperparameters for the W2V SVM model (load or compute)\n",
    "if grid_path.exists():\n",
    "    grid_search = joblib.load(grid_path)\n",
    "else:\n",
    "    svm_W2V = make_pipeline(StandardScaler(), SVC())\n",
    "    param_grid = {\n",
    "        \"svc__kernel\": [\"linear\", \"rbf\"],\n",
    "        \"svc__C\": [0.1, 1, 10],\n",
    "        \"svc__gamma\": [0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "    grid_search = GridSearchCV(svm_W2V, param_grid, cv=5, scoring='f1_macro')\n",
    "    grid_search.fit(sentence_embeddings_train, labels_train)\n",
    "    joblib.dump(grid_search, grid_path)\n",
    "\n",
    "print(grid_search.best_params_, grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.b. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "(FILES_DIR / \"joblib_files\").mkdir(parents=True, exist_ok=True)\n",
    "model_path = FILES_DIR / \"joblib_files\" / \"svm_W2V_model.joblib\"\n",
    "\n",
    "if model_path.exists():\n",
    "    svm_W2V = load(model_path)\n",
    "else:\n",
    "    svm_W2V = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(C=10, gamma=0.001, kernel='rbf', decision_function_shape='ovr')\n",
    "    )\n",
    "    svm_W2V.fit(sentence_embeddings_train, labels_train)\n",
    "    dump(svm_W2V, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.c. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved SVM model\n",
    "svm_W2V = load(FILES_DIR / \"joblib_files\" / \"svm_W2V_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM_W2V:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6e6de\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6e6de_level0_col0\" class=\"col_heading level0 col0\" >precision</th>\n",
       "      <th id=\"T_6e6de_level0_col1\" class=\"col_heading level0 col1\" >recall</th>\n",
       "      <th id=\"T_6e6de_level0_col2\" class=\"col_heading level0 col2\" >f1-score</th>\n",
       "      <th id=\"T_6e6de_level0_col3\" class=\"col_heading level0 col3\" >support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6e6de_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6e6de_row0_col0\" class=\"data row0 col0\" >0.8463</td>\n",
       "      <td id=\"T_6e6de_row0_col1\" class=\"data row0 col1\" >0.8955</td>\n",
       "      <td id=\"T_6e6de_row0_col2\" class=\"data row0 col2\" >0.8702</td>\n",
       "      <td id=\"T_6e6de_row0_col3\" class=\"data row0 col3\" >2958.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e6de_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6e6de_row1_col0\" class=\"data row1 col0\" >0.8386</td>\n",
       "      <td id=\"T_6e6de_row1_col1\" class=\"data row1 col1\" >0.8482</td>\n",
       "      <td id=\"T_6e6de_row1_col2\" class=\"data row1 col2\" >0.8434</td>\n",
       "      <td id=\"T_6e6de_row1_col3\" class=\"data row1 col3\" >2984.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e6de_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6e6de_row2_col0\" class=\"data row2 col0\" >0.8254</td>\n",
       "      <td id=\"T_6e6de_row2_col1\" class=\"data row2 col1\" >0.8254</td>\n",
       "      <td id=\"T_6e6de_row2_col2\" class=\"data row2 col2\" >0.8254</td>\n",
       "      <td id=\"T_6e6de_row2_col3\" class=\"data row2 col3\" >0.8254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e6de_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6e6de_row3_col0\" class=\"data row3 col0\" >0.8218</td>\n",
       "      <td id=\"T_6e6de_row3_col1\" class=\"data row3 col1\" >0.8254</td>\n",
       "      <td id=\"T_6e6de_row3_col2\" class=\"data row3 col2\" >0.8216</td>\n",
       "      <td id=\"T_6e6de_row3_col3\" class=\"data row3 col3\" >7040.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e6de_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6e6de_row4_col0\" class=\"data row4 col0\" >0.7635</td>\n",
       "      <td id=\"T_6e6de_row4_col1\" class=\"data row4 col1\" >0.6760</td>\n",
       "      <td id=\"T_6e6de_row4_col2\" class=\"data row4 col2\" >0.7104</td>\n",
       "      <td id=\"T_6e6de_row4_col3\" class=\"data row4 col3\" >7040.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e6de_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_6e6de_row5_col0\" class=\"data row5 col0\" >0.6868</td>\n",
       "      <td id=\"T_6e6de_row5_col1\" class=\"data row5 col1\" >0.6502</td>\n",
       "      <td id=\"T_6e6de_row5_col2\" class=\"data row5 col2\" >0.6680</td>\n",
       "      <td id=\"T_6e6de_row5_col3\" class=\"data row5 col3\" >506.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e6de_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_6e6de_row6_col0\" class=\"data row6 col0\" >0.7448</td>\n",
       "      <td id=\"T_6e6de_row6_col1\" class=\"data row6 col1\" >0.5476</td>\n",
       "      <td id=\"T_6e6de_row6_col2\" class=\"data row6 col2\" >0.6311</td>\n",
       "      <td id=\"T_6e6de_row6_col3\" class=\"data row6 col3\" >389.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6e6de_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_6e6de_row7_col0\" class=\"data row7 col0\" >0.7008</td>\n",
       "      <td id=\"T_6e6de_row7_col1\" class=\"data row7 col1\" >0.4384</td>\n",
       "      <td id=\"T_6e6de_row7_col2\" class=\"data row7 col2\" >0.5394</td>\n",
       "      <td id=\"T_6e6de_row7_col3\" class=\"data row7 col3\" >203.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e6c01734d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TESTING ON TEST SET\n",
    "y_pred = svm_W2V.predict(sentence_embeddings_test)\n",
    "y_test_np = y_test.cpu().numpy()\n",
    "report_dict_W2V = classification_report(y_test_np, y_pred, output_dict=True)\n",
    "report_W2V_SVM = pd.DataFrame(report_dict_W2V).transpose()\n",
    "\n",
    "print(\"Classification Report for SVM_W2V:\")\n",
    "evalkit.display_report(report_W2V_SVM, sort_col='f1-score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This is very negative sentence, omg, I hate it\n",
      "Predicted Class: Very positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' TESTING ON INPUT SENTENCE '''\n",
    "test_sentence = \"This is very negative sentence, omg, I hate it\"\n",
    "# test_sentence = \"The price is going down, you will lose all your money\"\n",
    "# test_sentence = \"Bitcoin price is increasing, well we have it guys, great job\"\n",
    "# test_sentence = \"omg bro this can't be real, we lost, price is very low\"\n",
    "\n",
    "# Tokenize, clean, and convert the test sentence to a numpy array\n",
    "test_sentence_token_words = preprocess_tokens(test_sentence)\n",
    "test_sentence_embedding = textprep.post_to_embedding(test_sentence_token_words, word2vec)\n",
    "# Predict the class\n",
    "predicted_class = svm_W2V.predict([test_sentence_embedding])[0]\n",
    "print(f\"Input: {test_sentence}\")\n",
    "print(f\"Predicted Class: {class_dict_bert_sentiment[predicted_class]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. BERT + MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.a. Multi-class classification\n",
    "- target variable: `BERT-Sentiment`\n",
    "- input: vector dim (128,1); sentence vector representation, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_MLP_1L_ReLU_MC_FT(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BERT_MLP_1L_ReLU_MC_FT, self).__init__()\n",
    "        self.bert = bert_model.to(device)  # Pre-trained BERT model\n",
    "        hidden_size = bert_model.config.hidden_size  # Typically 768 for BERT-base\n",
    "\n",
    "        # Define a 1-layer feedforward neural network\n",
    "        self.fc1 = nn.Linear(hidden_size, num_classes).to(device)  # Output layer only\n",
    "\n",
    "        # Activation function only, no dropout or batch normalization\n",
    "        self.activation = nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Fine-tuning BERT by allowing gradient updates\n",
    "        outputs = self.bert(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
    "\n",
    "        # Pass through the single-layer MLP\n",
    "        logits = self.fc1(pooled_output)  # Direct output without dropout\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_MLP_3L_ReLU_MC_FT(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BERT_MLP_3L_ReLU_MC_FT, self).__init__()\n",
    "        self.bert = bert_model.to(device)  # Pre-trained BERT model\n",
    "        hidden_size = bert_model.config.hidden_size  # Typically 768 for BERT-base\n",
    "\n",
    "        # Define a 3-layer feedforward neural network\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)  # Layer 1\n",
    "        self.fc2 = nn.Linear(512, 256).to(device)         # Layer 2\n",
    "        self.fc3 = nn.Linear(256, num_classes).to(device)  # Output Layer\n",
    "\n",
    "        # Activation function and dropout\n",
    "        self.activation = nn.ReLU().to(device)\n",
    "        self.dropout = nn.Dropout(0.3).to(device)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Fine-tuning BERT by allowing gradient updates\n",
    "        outputs = self.bert(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
    "\n",
    "        # Pass through the 3-layer MLP\n",
    "        x = self.dropout(self.activation(self.fc1(pooled_output)))  # Layer 1\n",
    "        x = self.dropout(self.activation(self.fc2(x)))              # Layer 2\n",
    "        logits = self.fc3(x)                                        # Output Layer\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_MLP_3L_ReLU_MC_BN_FT(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BERT_MLP_3L_ReLU_MC_BN_FT, self).__init__()\n",
    "        self.bert = bert_model.to(device)  # Pre-trained BERT model\n",
    "        hidden_size = bert_model.config.hidden_size  # Typically 768 for BERT-base\n",
    "\n",
    "        # Define a 3-layer feedforward neural network with BatchNorm\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)  # Layer 1\n",
    "        self.bn1 = nn.BatchNorm1d(512).to(device)          # BatchNorm after Layer 1\n",
    "        self.fc2 = nn.Linear(512, 256).to(device)          # Layer 2\n",
    "        self.bn2 = nn.BatchNorm1d(256).to(device)          # BatchNorm after Layer 2\n",
    "        self.fc3 = nn.Linear(256, num_classes).to(device)  # Output Layer\n",
    "\n",
    "        # Activation function and dropout\n",
    "        self.activation = nn.ReLU().to(device)\n",
    "        self.dropout = nn.Dropout(0.3).to(device)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Fine-tuning BERT by allowing gradient updates\n",
    "        outputs = self.bert(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
    "\n",
    "        # Pass through the 3-layer MLP with BatchNorm\n",
    "        x = self.dropout(self.activation(self.bn1(self.fc1(pooled_output))))  # Layer 1 with BatchNorm\n",
    "        x = self.dropout(self.activation(self.bn2(self.fc2(x))))              # Layer 2 with BatchNorm\n",
    "        logits = self.fc3(x)                                                 # Output Layer\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_MLP_5L_ReLU_MC_BN_FT(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BERT_MLP_5L_ReLU_MC_BN_FT, self).__init__()\n",
    "        self.bert = bert_model.to(device)  # Pre-trained BERT model\n",
    "        hidden_size = bert_model.config.hidden_size  # Typically 768 for BERT-base\n",
    "\n",
    "        # Define a 5-layer feedforward neural network\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)  # Layer 1\n",
    "        self.bn1 = nn.BatchNorm1d(512).to(device)\n",
    "        self.fc2 = nn.Linear(512, 256).to(device)         # Layer 2\n",
    "        self.bn2 = nn.BatchNorm1d(256).to(device)\n",
    "        self.fc3 = nn.Linear(256, 128).to(device)         # Layer 3\n",
    "        self.bn3 = nn.BatchNorm1d(128).to(device)\n",
    "        self.fc4 = nn.Linear(128, 64).to(device)          # Layer 4\n",
    "        self.bn4 = nn.BatchNorm1d(64).to(device)\n",
    "        self.fc5 = nn.Linear(64, num_classes).to(device)  # Output Layer\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = nn.ReLU().to(device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Fine-tuning BERT by allowing gradient updates\n",
    "        outputs = self.bert(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
    "\n",
    "        # Pass through the 5-layer MLP with Batch Normalization\n",
    "        x = self.activation(self.bn1(self.fc1(pooled_output)))  # Layer 1 with BN\n",
    "        x = self.activation(self.bn2(self.fc2(x)))              # Layer 2 with BN\n",
    "        x = self.activation(self.bn3(self.fc3(x)))              # Layer 3 with BN\n",
    "        x = self.activation(self.bn4(self.fc4(x)))              # Layer 4 with BN\n",
    "        logits = self.fc5(x)                                    # Output Layer (logits)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_MLP_5L_ReLU_MC_DO30_FT(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BERT_MLP_5L_ReLU_MC_DO30_FT, self).__init__()\n",
    "        self.bert = bert_model.to(device)  # Pre-trained BERT model\n",
    "        hidden_size = bert_model.config.hidden_size  # Typically 768 for BERT-base\n",
    "\n",
    "        # Define a 5-layer feedforward neural network\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)  # Layer 1\n",
    "        self.fc2 = nn.Linear(512, 256).to(device)         # Layer 2\n",
    "        self.fc3 = nn.Linear(256, 128).to(device)         # Layer 3\n",
    "        self.fc4 = nn.Linear(128, 64).to(device)          # Layer 4\n",
    "        self.fc5 = nn.Linear(64, num_classes).to(device)  # Output Layer\n",
    "\n",
    "        # Activation function and dropout\n",
    "        self.activation = nn.ReLU().to(device)\n",
    "        self.dropout = nn.Dropout(0.3).to(device)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Fine-tuning BERT by allowing gradient updates\n",
    "        outputs = self.bert(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
    "\n",
    "        # Pass through the 5-layer MLP\n",
    "        x = self.dropout(self.activation(self.fc1(pooled_output)))  # Layer 1\n",
    "        x = self.dropout(self.activation(self.fc2(x)))              # Layer 2\n",
    "        x = self.dropout(self.activation(self.fc3(x)))              # Layer 3\n",
    "        x = self.dropout(self.activation(self.fc4(x)))              # Layer 4\n",
    "        logits = self.fc5(x)                                        # Output Layer\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.b. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = [\n",
    "    # BERT_MLP_1L_ReLU_MC_FT(bert_model, num_classes=5).to(device),\n",
    "    # BERT_MLP_3L_ReLU_MC_FT(bert_model, num_classes=5).to(device),\n",
    "    # BERT_MLP_3L_ReLU_MC_BN_FT(bert_model, num_classes=5).to(device),\n",
    "    # BERT_MLP_5L_ReLU_MC_BN_FT(bert_model, num_classes=5).to(device),\n",
    "    # BERT_MLP_5L_ReLU_MC_DO30_FT(bert_model, num_classes=5).to(device),\n",
    "]\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "(FILES_DIR / \"pth_files\").mkdir(parents=True, exist_ok=True)\n",
    "(TSB_DIR / \"BERT_FT\").mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=str(TSB_DIR / \"BERT_FT\"))\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Unfreeze BERT parameters and set parameter groups\n",
    "def get_optimizer(model):\n",
    "    \"\"\"\n",
    "    Creates parameter groups so BERT layers get a smaller learning rate,\n",
    "    while the classifier layers get a slightly higher learning rate.\n",
    "    \"\"\"\n",
    "    for name, param in model.bert.named_parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    bert_params = []\n",
    "    classifier_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"bert\" in name:\n",
    "            bert_params.append(param)\n",
    "        else:\n",
    "            classifier_params.append(param)\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        [\n",
    "            {\"params\": bert_params, \"lr\": 2e-5, \"weight_decay\": 1e-5},\n",
    "            {\"params\": classifier_params, \"lr\": 1e-4, \"weight_decay\": 1e-5},\n",
    "        ]\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    avg_val_loss = val_loss / len(dataloader)\n",
    "    val_accuracy = correct / total\n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "# Training loop with TensorBoard and early stopping\n",
    "epochs = 5\n",
    "patience = 2\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    print(f\"Training {model_name}\")\n",
    "    optimizer = get_optimizer(model)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for input_ids, attention_mask, labels in tqdm(train_dataloader_bert, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader_bert)\n",
    "        val_loss, val_accuracy = validate_model(model, val_dataloader_bert, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Train loss: {avg_train_loss:.4f}  Validation loss: {val_loss:.4f}  Validation accuracy: {val_accuracy:.4f}\")\n",
    "        writer.add_scalar(f\"{model_name}/Train_Loss\", avg_train_loss, epoch)\n",
    "        writer.add_scalar(f\"{model_name}/Val_Loss\", val_loss, epoch)\n",
    "        writer.add_scalar(f\"{model_name}/Val_Accuracy\", val_accuracy, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement_count = 0\n",
    "            torch.save(model.state_dict(), FILES_DIR / \"pth_files\" / f\"{model_name}_best_weights.pth\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(FILES_DIR / \"pth_files\" / f\"{model_name}_best_weights.pth\", map_location=device))\n",
    "    test_loss, test_accuracy = validate_model(model, test_dataloader_bert, criterion, device)\n",
    "    print(f\"Test loss: {test_loss:.4f}, test accuracy: {test_accuracy:.4f}\\n\")\n",
    "    torch.save(model.state_dict(), FILES_DIR / \"pth_files\" / f\"{model_name}_final_weights.pth\")\n",
    "    print(f\"Saved final weights for {model_name}.\")\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.c. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CHOOSE MODEL TO TEST '''\n",
    "models = [\n",
    "    # BERT_MLP_1L_ReLU_MC_FT(bert_model, num_classes=5).to(device),\n",
    "    # BERT_MLP_3L_ReLU_MC_FT(bert_model, num_classes=5).to(device),\n",
    "    # BERT_MLP_3L_ReLU_MC_BN_FT(bert_model, num_classes=5).to(device),\n",
    "    # BERT_MLP_5L_ReLU_MC_BN_FT(bert_model, num_classes=5).to(device),\n",
    "    # BERT_MLP_5L_ReLU_MC_DO30_FT(bert_model, num_classes=5).to(device),\n",
    "]\n",
    "\n",
    "# Load the weights\n",
    "for model in models:\n",
    "    _best = FILES_DIR / \"pth_files\" / f\"{model.__class__.__name__}_best_weights.pth\"\n",
    "    _std  = FILES_DIR / \"pth_files\" / f\"{model.__class__.__name__}_weights.pth\"\n",
    "    _path = _best if _best.exists() else _std\n",
    "    model.load_state_dict(torch.load(_path, map_location=device, weights_only=False))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TESTING ON VALIDATION AND TEST SETS '''\n",
    "\n",
    "# Pack inputs for BERT models (expects dict with input_ids & attention_mask)\n",
    "X_val = {\"input_ids\": X_bert_val[\"input_ids\"], \"attention_mask\": X_bert_val[\"attention_mask\"]}\n",
    "X_test = {\"input_ids\": X_bert_test[\"input_ids\"], \"attention_mask\": X_bert_test[\"attention_mask\"]}\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n--- {model.__class__.__name__} ---\")\n",
    "    evalkit.evaluate(model, X_val, y_val, dataset_name=\"Validation\", device=device)\n",
    "    evalkit.evaluate(model, X_test, y_test, dataset_name=\"Test\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TESTING ON INPUT SENTENCE'''\n",
    "test_sentence = \"This is very negative sentence, omg, I hate it\"\n",
    "# test_sentence = \"The price is going down, you will lose all your money\"\n",
    "# test_sentence = \"Bitcoin is going up guys, we will be rich, nice\"\n",
    "# test_sentence = \"omg bro this can't be real, we lost, price is very low\"\n",
    "\n",
    "encoded_input = textprep.bert_tokenize_function(test_sentence, bert_tokenizer)\n",
    "input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n--- {model.__class__.__name__} ---\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pred = torch.argmax(torch.softmax(logits, dim=1), dim=1).item()\n",
    "    sentiment = class_dict_bert_sentiment[pred]\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. BERT + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.a. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svc__C': 10, 'svc__gamma': 0.001, 'svc__kernel': 'rbf'} 0.2034304271348117\n"
     ]
    }
   ],
   "source": [
    "pkl_dir = FILES_DIR / \"pkl_files\"\n",
    "pkl_dir.mkdir(parents=True, exist_ok=True)\n",
    "grid_path = pkl_dir / \"svm_BERT_grid_search.pkl\"\n",
    "\n",
    "# Find the best hyperparameters for the BERT SVM model (load or compute)\n",
    "if grid_path.exists():\n",
    "    grid_search_BERT = joblib.load(grid_path)\n",
    "else:\n",
    "    svm_BERT = make_pipeline(StandardScaler(), SVC())\n",
    "    param_grid = {\n",
    "        \"svc__kernel\": [\"rbf\"],\n",
    "        \"svc__C\": [0.1, 1, 10],\n",
    "        \"svc__gamma\": [0.001, 0.01, 0.1, 1],\n",
    "    }\n",
    "    grid_search_BERT = GridSearchCV(svm_BERT, param_grid, cv=5, scoring=\"f1_macro\")\n",
    "    grid_search_BERT.fit(X_bert_post_embeddings_train, y_train.cpu().numpy())\n",
    "    joblib.dump(grid_search_BERT, grid_path)\n",
    "\n",
    "print(grid_search_BERT.best_params_, grid_search_BERT.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.b. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "(FILES_DIR / \"joblib_files\").mkdir(parents=True, exist_ok=True)\n",
    "model_path = FILES_DIR / \"joblib_files\" / \"svm_BERT_model.joblib\"\n",
    "\n",
    "if model_path.exists():\n",
    "    svm_BERT = load(model_path)\n",
    "else:\n",
    "    svm_BERT = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(C=10, gamma=0.001, kernel='rbf', decision_function_shape='ovr')\n",
    "    )\n",
    "    svm_BERT.fit(X_bert_post_embeddings_train, y_train.cpu().numpy())\n",
    "    dump(svm_BERT, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.c. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved SVM model\n",
    "svm_BERT = load(FILES_DIR / \"joblib_files\" / \"svm_BERT_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM_BERT:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8dd1b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8dd1b_level0_col0\" class=\"col_heading level0 col0\" >precision</th>\n",
       "      <th id=\"T_8dd1b_level0_col1\" class=\"col_heading level0 col1\" >recall</th>\n",
       "      <th id=\"T_8dd1b_level0_col2\" class=\"col_heading level0 col2\" >f1-score</th>\n",
       "      <th id=\"T_8dd1b_level0_col3\" class=\"col_heading level0 col3\" >support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8dd1b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8dd1b_row0_col0\" class=\"data row0 col0\" >0.4563</td>\n",
       "      <td id=\"T_8dd1b_row0_col1\" class=\"data row0 col1\" >0.5235</td>\n",
       "      <td id=\"T_8dd1b_row0_col2\" class=\"data row0 col2\" >0.4876</td>\n",
       "      <td id=\"T_8dd1b_row0_col3\" class=\"data row0 col3\" >2984.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8dd1b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8dd1b_row1_col0\" class=\"data row1 col0\" >0.4513</td>\n",
       "      <td id=\"T_8dd1b_row1_col1\" class=\"data row1 col1\" >0.5237</td>\n",
       "      <td id=\"T_8dd1b_row1_col2\" class=\"data row1 col2\" >0.4848</td>\n",
       "      <td id=\"T_8dd1b_row1_col3\" class=\"data row1 col3\" >2958.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8dd1b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8dd1b_row2_col0\" class=\"data row2 col0\" >0.4433</td>\n",
       "      <td id=\"T_8dd1b_row2_col1\" class=\"data row2 col1\" >0.4433</td>\n",
       "      <td id=\"T_8dd1b_row2_col2\" class=\"data row2 col2\" >0.4433</td>\n",
       "      <td id=\"T_8dd1b_row2_col3\" class=\"data row2 col3\" >0.4433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8dd1b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8dd1b_row3_col0\" class=\"data row3 col0\" >0.3914</td>\n",
       "      <td id=\"T_8dd1b_row3_col1\" class=\"data row3 col1\" >0.4433</td>\n",
       "      <td id=\"T_8dd1b_row3_col2\" class=\"data row3 col2\" >0.4128</td>\n",
       "      <td id=\"T_8dd1b_row3_col3\" class=\"data row3 col3\" >7040.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8dd1b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_8dd1b_row4_col0\" class=\"data row4 col0\" >0.2108</td>\n",
       "      <td id=\"T_8dd1b_row4_col1\" class=\"data row4 col1\" >0.2142</td>\n",
       "      <td id=\"T_8dd1b_row4_col2\" class=\"data row4 col2\" >0.2027</td>\n",
       "      <td id=\"T_8dd1b_row4_col3\" class=\"data row4 col3\" >7040.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8dd1b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_8dd1b_row5_col0\" class=\"data row5 col0\" >0.0778</td>\n",
       "      <td id=\"T_8dd1b_row5_col1\" class=\"data row5 col1\" >0.0138</td>\n",
       "      <td id=\"T_8dd1b_row5_col2\" class=\"data row5 col2\" >0.0235</td>\n",
       "      <td id=\"T_8dd1b_row5_col3\" class=\"data row5 col3\" >506.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8dd1b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_8dd1b_row6_col0\" class=\"data row6 col0\" >0.0400</td>\n",
       "      <td id=\"T_8dd1b_row6_col1\" class=\"data row6 col1\" >0.0049</td>\n",
       "      <td id=\"T_8dd1b_row6_col2\" class=\"data row6 col2\" >0.0088</td>\n",
       "      <td id=\"T_8dd1b_row6_col3\" class=\"data row6 col3\" >203.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8dd1b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_8dd1b_row7_col0\" class=\"data row7 col0\" >0.0286</td>\n",
       "      <td id=\"T_8dd1b_row7_col1\" class=\"data row7 col1\" >0.0051</td>\n",
       "      <td id=\"T_8dd1b_row7_col2\" class=\"data row7 col2\" >0.0087</td>\n",
       "      <td id=\"T_8dd1b_row7_col3\" class=\"data row7 col3\" >389.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e78a241550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TESTING ON TEST SET\n",
    "y_pred = svm_BERT.predict(X_bert_post_embeddings_test)\n",
    "y_test_np = y_test.cpu().numpy()\n",
    "report_dict_BERT = classification_report(y_test_np, y_pred, output_dict=True)\n",
    "report_BERT_SVM = pd.DataFrame(report_dict_BERT).transpose()\n",
    "\n",
    "print(\"Classification Report for SVM_BERT:\")\n",
    "evalkit.display_report(report_BERT_SVM, sort_col='f1-score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This is very negative sentence, omg, I hate it\n",
      "Predicted Class: Very negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' TESTING ON INPUT SENTENCE '''\n",
    "test_sentence = \"This is very negative sentence, omg, I hate it\"\n",
    "# test_sentence = \"The price is going down, you will lose all your money\"\n",
    "# test_sentence = \"Bitcoin price is increasing, well we have it guys, great job\"\n",
    "# test_sentence = \"omg bro this can't be real, we lost, price is very low\"\n",
    "\n",
    "# Get BERT embeddings for the test sentence\n",
    "test_sentence_token_words = textprep.bert_tokenize_function(test_sentence, bert_tokenizer)\n",
    "test_sentence_embedding = textprep.get_bert_embedding(test_sentence_token_words, model=bert_model, device=device)\n",
    "\n",
    "# Predict the class\n",
    "predicted_class = svm_BERT.predict(test_sentence_embedding)[0]\n",
    "print(f\"Input: {test_sentence}\")\n",
    "print(f\"Predicted Class: {class_dict_bert_sentiment[predicted_class]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.a. Classification reports generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_94dd6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_94dd6_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_94dd6_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_94dd6_level0_col2\" class=\"col_heading level0 col2\" >Macro Avg F1-Score</th>\n",
       "      <th id=\"T_94dd6_level0_col3\" class=\"col_heading level0 col3\" >Weighted Avg F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_94dd6_row0_col0\" class=\"data row0 col0\" >W2V_SVM_C_10_gamma_0.001_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row0_col1\" class=\"data row0 col1\" >0.7679</td>\n",
       "      <td id=\"T_94dd6_row0_col2\" class=\"data row0 col2\" >0.5677</td>\n",
       "      <td id=\"T_94dd6_row0_col3\" class=\"data row0 col3\" >0.7613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_94dd6_row1_col0\" class=\"data row1 col0\" >W2V_SVM_C_10_gamma_0.01_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row1_col1\" class=\"data row1 col1\" >0.7744</td>\n",
       "      <td id=\"T_94dd6_row1_col2\" class=\"data row1 col2\" >0.5551</td>\n",
       "      <td id=\"T_94dd6_row1_col3\" class=\"data row1 col3\" >0.7579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_94dd6_row2_col0\" class=\"data row2 col0\" >W2V_SVM_C_1_gamma_0.01_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row2_col1\" class=\"data row2 col1\" >0.7591</td>\n",
       "      <td id=\"T_94dd6_row2_col2\" class=\"data row2 col2\" >0.5183</td>\n",
       "      <td id=\"T_94dd6_row2_col3\" class=\"data row2 col3\" >0.7387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_94dd6_row3_col0\" class=\"data row3 col0\" >W2V_SVM_C_1_gamma_0.001_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row3_col1\" class=\"data row3 col1\" >0.7438</td>\n",
       "      <td id=\"T_94dd6_row3_col2\" class=\"data row3 col2\" >0.4880</td>\n",
       "      <td id=\"T_94dd6_row3_col3\" class=\"data row3 col3\" >0.7247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_94dd6_row4_col0\" class=\"data row4 col0\" >W2V_SVM_C_0.1_gamma_1_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row4_col1\" class=\"data row4 col1\" >0.7236</td>\n",
       "      <td id=\"T_94dd6_row4_col2\" class=\"data row4 col2\" >0.5152</td>\n",
       "      <td id=\"T_94dd6_row4_col3\" class=\"data row4 col3\" >0.7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_94dd6_row5_col0\" class=\"data row5 col0\" >W2V_SVM_C_0.1_gamma_0.001_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row5_col1\" class=\"data row5 col1\" >0.7236</td>\n",
       "      <td id=\"T_94dd6_row5_col2\" class=\"data row5 col2\" >0.5152</td>\n",
       "      <td id=\"T_94dd6_row5_col3\" class=\"data row5 col3\" >0.7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_94dd6_row6_col0\" class=\"data row6 col0\" >W2V_SVM_C_0.1_gamma_0.1_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row6_col1\" class=\"data row6 col1\" >0.7236</td>\n",
       "      <td id=\"T_94dd6_row6_col2\" class=\"data row6 col2\" >0.5152</td>\n",
       "      <td id=\"T_94dd6_row6_col3\" class=\"data row6 col3\" >0.7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_94dd6_row7_col0\" class=\"data row7 col0\" >W2V_SVM_C_0.1_gamma_0.01_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row7_col1\" class=\"data row7 col1\" >0.7236</td>\n",
       "      <td id=\"T_94dd6_row7_col2\" class=\"data row7 col2\" >0.5152</td>\n",
       "      <td id=\"T_94dd6_row7_col3\" class=\"data row7 col3\" >0.7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_94dd6_row8_col0\" class=\"data row8 col0\" >W2V_SVM_C_1_gamma_0.001_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row8_col1\" class=\"data row8 col1\" >0.7182</td>\n",
       "      <td id=\"T_94dd6_row8_col2\" class=\"data row8 col2\" >0.5058</td>\n",
       "      <td id=\"T_94dd6_row8_col3\" class=\"data row8 col3\" >0.7103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_94dd6_row9_col0\" class=\"data row9 col0\" >W2V_SVM_C_1_gamma_0.01_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row9_col1\" class=\"data row9 col1\" >0.7182</td>\n",
       "      <td id=\"T_94dd6_row9_col2\" class=\"data row9 col2\" >0.5058</td>\n",
       "      <td id=\"T_94dd6_row9_col3\" class=\"data row9 col3\" >0.7103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_94dd6_row10_col0\" class=\"data row10 col0\" >W2V_SVM_C_1_gamma_0.1_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row10_col1\" class=\"data row10 col1\" >0.7182</td>\n",
       "      <td id=\"T_94dd6_row10_col2\" class=\"data row10 col2\" >0.5058</td>\n",
       "      <td id=\"T_94dd6_row10_col3\" class=\"data row10 col3\" >0.7103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_94dd6_row11_col0\" class=\"data row11 col0\" >W2V_SVM_C_1_gamma_1_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row11_col1\" class=\"data row11 col1\" >0.7182</td>\n",
       "      <td id=\"T_94dd6_row11_col2\" class=\"data row11 col2\" >0.5058</td>\n",
       "      <td id=\"T_94dd6_row11_col3\" class=\"data row11 col3\" >0.7103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_94dd6_row12_col0\" class=\"data row12 col0\" >W2V_SVM_C_10_gamma_0.001_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row12_col1\" class=\"data row12 col1\" >0.7185</td>\n",
       "      <td id=\"T_94dd6_row12_col2\" class=\"data row12 col2\" >0.5053</td>\n",
       "      <td id=\"T_94dd6_row12_col3\" class=\"data row12 col3\" >0.7102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_94dd6_row13_col0\" class=\"data row13 col0\" >W2V_SVM_C_10_gamma_0.01_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row13_col1\" class=\"data row13 col1\" >0.7185</td>\n",
       "      <td id=\"T_94dd6_row13_col2\" class=\"data row13 col2\" >0.5053</td>\n",
       "      <td id=\"T_94dd6_row13_col3\" class=\"data row13 col3\" >0.7102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_94dd6_row14_col0\" class=\"data row14 col0\" >W2V_SVM_C_10_gamma_0.1_kernel_linear.csv</td>\n",
       "      <td id=\"T_94dd6_row14_col1\" class=\"data row14 col1\" >0.7185</td>\n",
       "      <td id=\"T_94dd6_row14_col2\" class=\"data row14 col2\" >0.5053</td>\n",
       "      <td id=\"T_94dd6_row14_col3\" class=\"data row14 col3\" >0.7102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_94dd6_row15_col0\" class=\"data row15 col0\" >W2V_SVM_C_10_gamma_0.1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row15_col1\" class=\"data row15 col1\" >0.7065</td>\n",
       "      <td id=\"T_94dd6_row15_col2\" class=\"data row15 col2\" >0.4548</td>\n",
       "      <td id=\"T_94dd6_row15_col3\" class=\"data row15 col3\" >0.6750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_94dd6_row16_col0\" class=\"data row16 col0\" >W2V_SVM_C_1_gamma_0.1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row16_col1\" class=\"data row16 col1\" >0.6997</td>\n",
       "      <td id=\"T_94dd6_row16_col2\" class=\"data row16 col2\" >0.4399</td>\n",
       "      <td id=\"T_94dd6_row16_col3\" class=\"data row16 col3\" >0.6666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_94dd6_row17_col0\" class=\"data row17 col0\" >W2V_SVM_C_1_gamma_1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row17_col1\" class=\"data row17 col1\" >0.6759</td>\n",
       "      <td id=\"T_94dd6_row17_col2\" class=\"data row17 col2\" >0.4286</td>\n",
       "      <td id=\"T_94dd6_row17_col3\" class=\"data row17 col3\" >0.6406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_94dd6_row18_col0\" class=\"data row18 col0\" >W2V_SVM_C_0.1_gamma_0.001_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row18_col1\" class=\"data row18 col1\" >0.6876</td>\n",
       "      <td id=\"T_94dd6_row18_col2\" class=\"data row18 col2\" >0.3014</td>\n",
       "      <td id=\"T_94dd6_row18_col3\" class=\"data row18 col3\" >0.6304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_94dd6_row19_col0\" class=\"data row19 col0\" >W2V_SVM_C_0.1_gamma_0.01_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row19_col1\" class=\"data row19 col1\" >0.6653</td>\n",
       "      <td id=\"T_94dd6_row19_col2\" class=\"data row19 col2\" >0.3437</td>\n",
       "      <td id=\"T_94dd6_row19_col3\" class=\"data row19 col3\" >0.6226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_94dd6_row20_col0\" class=\"data row20 col0\" >W2V_SVM_C_0.1_gamma_0.1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row20_col1\" class=\"data row20 col1\" >0.5699</td>\n",
       "      <td id=\"T_94dd6_row20_col2\" class=\"data row20 col2\" >0.2575</td>\n",
       "      <td id=\"T_94dd6_row20_col3\" class=\"data row20 col3\" >0.5016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94dd6_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_94dd6_row21_col0\" class=\"data row21 col0\" >W2V_SVM_C_0.1_gamma_1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_94dd6_row21_col1\" class=\"data row21 col1\" >0.5555</td>\n",
       "      <td id=\"T_94dd6_row21_col2\" class=\"data row21 col2\" >0.2481</td>\n",
       "      <td id=\"T_94dd6_row21_col3\" class=\"data row21 col3\" >0.4816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e6c05f4790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' GENERATE CLASSIFICATION REPORTS FOR ALL W2V SVM MODELS '''\n",
    "reports_dir = FILES_DIR / \"reports\" / \"classification_reports_w2v_svm\"\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "grid_search = joblib.load(FILES_DIR / \"pkl_files\" / \"svm_W2V_grid_search.pkl\")\n",
    "\n",
    "summary_csv = reports_dir / \"W2V_SVM_summary_classification_report.csv\"\n",
    "if summary_csv.exists():\n",
    "    summary_df = pd.read_csv(summary_csv)\n",
    "else:\n",
    "    summary_df = evalkit.generate_svm_reports(\n",
    "        X_train=sentence_embeddings_train,\n",
    "        y_train=y_train,\n",
    "        X_test=sentence_embeddings_test,\n",
    "        y_test=y_test,\n",
    "        output_dir=reports_dir,\n",
    "        prefix=\"W2V_SVM\",\n",
    "        grid_search=grid_search\n",
    "    )\n",
    "\n",
    "evalkit.display_report(summary_df, sort_col=\"Weighted Avg F1-Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_58e73\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_58e73_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_58e73_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_58e73_level0_col2\" class=\"col_heading level0 col2\" >Macro Avg F1-Score</th>\n",
       "      <th id=\"T_58e73_level0_col3\" class=\"col_heading level0 col3\" >Weighted Avg F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_58e73_row0_col0\" class=\"data row0 col0\" >BERT_SVM_C_1_gamma_0.001_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row0_col1\" class=\"data row0 col1\" >0.4538</td>\n",
       "      <td id=\"T_58e73_row0_col2\" class=\"data row0 col2\" >0.1968</td>\n",
       "      <td id=\"T_58e73_row0_col3\" class=\"data row0 col3\" >0.4152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_58e73_row1_col0\" class=\"data row1 col0\" >BERT_SVM_C_10_gamma_0.001_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row1_col1\" class=\"data row1 col1\" >0.4433</td>\n",
       "      <td id=\"T_58e73_row1_col2\" class=\"data row1 col2\" >0.2027</td>\n",
       "      <td id=\"T_58e73_row1_col3\" class=\"data row1 col3\" >0.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_58e73_row2_col0\" class=\"data row2 col0\" >BERT_SVM_C_1_gamma_0.01_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row2_col1\" class=\"data row2 col1\" >0.4536</td>\n",
       "      <td id=\"T_58e73_row2_col2\" class=\"data row2 col2\" >0.1953</td>\n",
       "      <td id=\"T_58e73_row2_col3\" class=\"data row2 col3\" >0.4024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_58e73_row3_col0\" class=\"data row3 col0\" >BERT_SVM_C_10_gamma_0.01_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row3_col1\" class=\"data row3 col1\" >0.4449</td>\n",
       "      <td id=\"T_58e73_row3_col2\" class=\"data row3 col2\" >0.1976</td>\n",
       "      <td id=\"T_58e73_row3_col3\" class=\"data row3 col3\" >0.4006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_58e73_row4_col0\" class=\"data row4 col0\" >BERT_SVM_C_0.1_gamma_0.001_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row4_col1\" class=\"data row4 col1\" >0.4550</td>\n",
       "      <td id=\"T_58e73_row4_col2\" class=\"data row4 col2\" >0.1883</td>\n",
       "      <td id=\"T_58e73_row4_col3\" class=\"data row4 col3\" >0.3976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_58e73_row5_col0\" class=\"data row5 col0\" >BERT_SVM_C_10_gamma_0.1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row5_col1\" class=\"data row5 col1\" >0.4520</td>\n",
       "      <td id=\"T_58e73_row5_col2\" class=\"data row5 col2\" >0.1924</td>\n",
       "      <td id=\"T_58e73_row5_col3\" class=\"data row5 col3\" >0.3893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_58e73_row6_col0\" class=\"data row6 col0\" >BERT_SVM_C_1_gamma_0.1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row6_col1\" class=\"data row6 col1\" >0.4516</td>\n",
       "      <td id=\"T_58e73_row6_col2\" class=\"data row6 col2\" >0.1911</td>\n",
       "      <td id=\"T_58e73_row6_col3\" class=\"data row6 col3\" >0.3877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_58e73_row7_col0\" class=\"data row7 col0\" >BERT_SVM_C_1_gamma_1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row7_col1\" class=\"data row7 col1\" >0.4501</td>\n",
       "      <td id=\"T_58e73_row7_col2\" class=\"data row7 col2\" >0.1889</td>\n",
       "      <td id=\"T_58e73_row7_col3\" class=\"data row7 col3\" >0.3832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_58e73_row8_col0\" class=\"data row8 col0\" >BERT_SVM_C_10_gamma_1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row8_col1\" class=\"data row8 col1\" >0.4497</td>\n",
       "      <td id=\"T_58e73_row8_col2\" class=\"data row8 col2\" >0.1889</td>\n",
       "      <td id=\"T_58e73_row8_col3\" class=\"data row8 col3\" >0.3832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_58e73_row9_col0\" class=\"data row9 col0\" >BERT_SVM_C_0.1_gamma_0.1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row9_col1\" class=\"data row9 col1\" >0.4547</td>\n",
       "      <td id=\"T_58e73_row9_col2\" class=\"data row9 col2\" >0.1656</td>\n",
       "      <td id=\"T_58e73_row9_col3\" class=\"data row9 col3\" >0.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_58e73_row10_col0\" class=\"data row10 col0\" >BERT_SVM_C_0.1_gamma_0.01_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row10_col1\" class=\"data row10 col1\" >0.4540</td>\n",
       "      <td id=\"T_58e73_row10_col2\" class=\"data row10 col2\" >0.1652</td>\n",
       "      <td id=\"T_58e73_row10_col3\" class=\"data row10 col3\" >0.3493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58e73_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_58e73_row11_col0\" class=\"data row11 col0\" >BERT_SVM_C_0.1_gamma_1_kernel_rbf.csv</td>\n",
       "      <td id=\"T_58e73_row11_col1\" class=\"data row11 col1\" >0.4530</td>\n",
       "      <td id=\"T_58e73_row11_col2\" class=\"data row11 col2\" >0.1635</td>\n",
       "      <td id=\"T_58e73_row11_col3\" class=\"data row11 col3\" >0.3457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e6c05fe410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' GENERATE CLASSIFICATION REPORTS FOR ALL BERT SVM MODELS '''\n",
    "reports_dir = FILES_DIR / \"reports\" / \"classification_reports_bert_svm\"\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "grid_search = joblib.load(FILES_DIR / \"pkl_files\" / \"svm_BERT_grid_search.pkl\")\n",
    "\n",
    "summary_csv = reports_dir / \"BERT_SVM_summary_classification_report.csv\"\n",
    "if summary_csv.exists():\n",
    "    summary_df = pd.read_csv(summary_csv)\n",
    "else:\n",
    "    summary_df = evalkit.generate_svm_reports(\n",
    "        X_train=X_bert_post_embeddings_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_bert_post_embeddings_test,\n",
    "        y_test=y_test,\n",
    "        output_dir=reports_dir,\n",
    "        prefix=\"BERT_SVM\",\n",
    "        grid_search=grid_search\n",
    "    )\n",
    "\n",
    "evalkit.display_report(summary_df, sort_col=\"Weighted Avg F1-Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5ad75\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5ad75_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_5ad75_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_5ad75_level0_col2\" class=\"col_heading level0 col2\" >Macro Avg F1-Score</th>\n",
       "      <th id=\"T_5ad75_level0_col3\" class=\"col_heading level0 col3\" >Weighted Avg F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5ad75_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5ad75_row0_col0\" class=\"data row0 col0\" >W2V_MLP_5L_ReLU_MC_BN4</td>\n",
       "      <td id=\"T_5ad75_row0_col1\" class=\"data row0 col1\" >0.7747</td>\n",
       "      <td id=\"T_5ad75_row0_col2\" class=\"data row0 col2\" >0.5215</td>\n",
       "      <td id=\"T_5ad75_row0_col3\" class=\"data row0 col3\" >0.7579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ad75_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5ad75_row1_col0\" class=\"data row1 col0\" >W2V_MLP_5L_ReLU_MC_BN2</td>\n",
       "      <td id=\"T_5ad75_row1_col1\" class=\"data row1 col1\" >0.7520</td>\n",
       "      <td id=\"T_5ad75_row1_col2\" class=\"data row1 col2\" >0.5044</td>\n",
       "      <td id=\"T_5ad75_row1_col3\" class=\"data row1 col3\" >0.7373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ad75_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5ad75_row2_col0\" class=\"data row2 col0\" >W2V_MLP_3L_ReLU_MC_DO30</td>\n",
       "      <td id=\"T_5ad75_row2_col1\" class=\"data row2 col1\" >0.7138</td>\n",
       "      <td id=\"T_5ad75_row2_col2\" class=\"data row2 col2\" >0.3097</td>\n",
       "      <td id=\"T_5ad75_row2_col3\" class=\"data row2 col3\" >0.6535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ad75_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5ad75_row3_col0\" class=\"data row3 col0\" >W2V_MLP_5L_ReLU_MC</td>\n",
       "      <td id=\"T_5ad75_row3_col1\" class=\"data row3 col1\" >0.7060</td>\n",
       "      <td id=\"T_5ad75_row3_col2\" class=\"data row3 col2\" >0.3063</td>\n",
       "      <td id=\"T_5ad75_row3_col3\" class=\"data row3 col3\" >0.6463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ad75_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5ad75_row4_col0\" class=\"data row4 col0\" >W2V_MLP_3L_ReLU_MC</td>\n",
       "      <td id=\"T_5ad75_row4_col1\" class=\"data row4 col1\" >0.7048</td>\n",
       "      <td id=\"T_5ad75_row4_col2\" class=\"data row4 col2\" >0.3058</td>\n",
       "      <td id=\"T_5ad75_row4_col3\" class=\"data row4 col3\" >0.6452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ad75_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_5ad75_row5_col0\" class=\"data row5 col0\" >W2V_MLP_1L_ReLU_MC</td>\n",
       "      <td id=\"T_5ad75_row5_col1\" class=\"data row5 col1\" >0.6798</td>\n",
       "      <td id=\"T_5ad75_row5_col2\" class=\"data row5 col2\" >0.2951</td>\n",
       "      <td id=\"T_5ad75_row5_col3\" class=\"data row5 col3\" >0.6226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e8089834d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' GENERATE CLASSIFICATION REPORTS FOR ALL W2V MLP MODELS '''\n",
    "models = [\n",
    "    (\"W2V_MLP_1L_ReLU_MC\", W2V_MLP_1L_ReLU_MC().to(device)),\n",
    "    (\"W2V_MLP_3L_ReLU_MC\", W2V_MLP_3L_ReLU_MC().to(device)),\n",
    "    (\"W2V_MLP_3L_ReLU_MC_DO30\", W2V_MLP_3L_ReLU_MC_DO30().to(device)),\n",
    "    (\"W2V_MLP_5L_ReLU_MC\", W2V_MLP_5L_ReLU_MC().to(device)),\n",
    "    (\"W2V_MLP_5L_ReLU_MC_BN2\", W2V_MLP_5L_ReLU_MC_BN2().to(device)),\n",
    "    (\"W2V_MLP_5L_ReLU_MC_BN4\", W2V_MLP_5L_ReLU_MC_BN4().to(device)),\n",
    "]\n",
    "\n",
    "reports_dir = FILES_DIR / \"reports\" / \"classification_reports_w2v_mlp\"\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "summary_csv = reports_dir.parent / \"W2V_MLP_summary_classification_report.csv\"\n",
    "\n",
    "if summary_csv.exists():\n",
    "    summary_df = pd.read_csv(summary_csv)\n",
    "else:\n",
    "    summary_df = evalkit.generate_mlp_reports(\n",
    "        models=models,\n",
    "        dataloader=test_dataloader,\n",
    "        device=device,\n",
    "        output_dir=reports_dir,\n",
    "        prefix=\"W2V_MLP\",\n",
    "        weight_dir=FILES_DIR / \"pth_files\"\n",
    "    )\n",
    "\n",
    "evalkit.display_report(summary_df, sort_col=\"Weighted Avg F1-Score\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_22ac1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_22ac1_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_22ac1_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_22ac1_level0_col2\" class=\"col_heading level0 col2\" >Macro Avg F1-Score</th>\n",
       "      <th id=\"T_22ac1_level0_col3\" class=\"col_heading level0 col3\" >Weighted Avg F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_22ac1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_22ac1_row0_col0\" class=\"data row0 col0\" >BERT_MLP_3L_ReLU_MC_FT</td>\n",
       "      <td id=\"T_22ac1_row0_col1\" class=\"data row0 col1\" >0.8513</td>\n",
       "      <td id=\"T_22ac1_row0_col2\" class=\"data row0 col2\" >0.7324</td>\n",
       "      <td id=\"T_22ac1_row0_col3\" class=\"data row0 col3\" >0.8498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22ac1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_22ac1_row1_col0\" class=\"data row1 col0\" >BERT_MLP_5L_ReLU_MC_BN_FT</td>\n",
       "      <td id=\"T_22ac1_row1_col1\" class=\"data row1 col1\" >0.8528</td>\n",
       "      <td id=\"T_22ac1_row1_col2\" class=\"data row1 col2\" >0.7268</td>\n",
       "      <td id=\"T_22ac1_row1_col3\" class=\"data row1 col3\" >0.8494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22ac1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_22ac1_row2_col0\" class=\"data row2 col0\" >BERT_MLP_3L_ReLU_MC_BN_FT</td>\n",
       "      <td id=\"T_22ac1_row2_col1\" class=\"data row2 col1\" >0.8322</td>\n",
       "      <td id=\"T_22ac1_row2_col2\" class=\"data row2 col2\" >0.7193</td>\n",
       "      <td id=\"T_22ac1_row2_col3\" class=\"data row2 col3\" >0.8304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_22ac1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_22ac1_row3_col0\" class=\"data row3 col0\" >BERT_MLP_1L_ReLU_MC_FT</td>\n",
       "      <td id=\"T_22ac1_row3_col1\" class=\"data row3 col1\" >0.8220</td>\n",
       "      <td id=\"T_22ac1_row3_col2\" class=\"data row3 col2\" >0.6279</td>\n",
       "      <td id=\"T_22ac1_row3_col3\" class=\"data row3 col3\" >0.8102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e6c05fee50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' GENERATE CLASSIFICATION REPORTS FOR ALL BERT MLP MODELS '''\n",
    "models = [\n",
    "    (\"BERT_MLP_1L_ReLU_MC_FT\",  BERT_MLP_1L_ReLU_MC_FT(bert_model, num_classes=5).to(device)),\n",
    "    (\"BERT_MLP_3L_ReLU_MC_FT\",  BERT_MLP_3L_ReLU_MC_FT(bert_model, num_classes=5).to(device)),\n",
    "    (\"BERT_MLP_3L_ReLU_MC_BN_FT\", BERT_MLP_3L_ReLU_MC_BN_FT(bert_model, num_classes=5).to(device)),\n",
    "    (\"BERT_MLP_5L_ReLU_MC_BN_FT\", BERT_MLP_5L_ReLU_MC_BN_FT(bert_model, num_classes=5).to(device)),\n",
    "]\n",
    "\n",
    "reports_dir = FILES_DIR / \"reports\" / \"classification_reports_bert_mlp\"\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "summary_csv = reports_dir / \"BERT_MLP_summary_classification_report.csv\"\n",
    "\n",
    "if summary_csv.exists():\n",
    "    summary_df = pd.read_csv(summary_csv)\n",
    "else:\n",
    "    summary_df = evalkit.generate_mlp_reports(\n",
    "        models=models,\n",
    "        dataloader=test_dataloader_bert,\n",
    "        device=device,\n",
    "        output_dir=reports_dir,\n",
    "        prefix=\"BERT_MLP\",\n",
    "        weight_dir=FILES_DIR / \"pth_files\"\n",
    "    )\n",
    "\n",
    "evalkit.display_report(summary_df, sort_col=\"Weighted Avg F1-Score\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.b. Architectures comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_26e81 th.col_heading.level0 {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_26e81 th.col_heading.level1 {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_26e81 th.row_heading {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_26e81\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_26e81_level0_col0\" class=\"col_heading level0 col0\" >Support</th>\n",
       "      <th id=\"T_26e81_level0_col1\" class=\"col_heading level0 col1\" colspan=\"3\">W2V_SVM</th>\n",
       "      <th id=\"T_26e81_level0_col4\" class=\"col_heading level0 col4\" colspan=\"3\">W2V_MLP</th>\n",
       "      <th id=\"T_26e81_level0_col7\" class=\"col_heading level0 col7\" colspan=\"3\">BERT_SVM</th>\n",
       "      <th id=\"T_26e81_level0_col10\" class=\"col_heading level0 col10\" colspan=\"3\">BERT_MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"blank level1\" >&nbsp;</th>\n",
       "      <th id=\"T_26e81_level1_col0\" class=\"col_heading level1 col0\" ></th>\n",
       "      <th id=\"T_26e81_level1_col1\" class=\"col_heading level1 col1\" >Precision</th>\n",
       "      <th id=\"T_26e81_level1_col2\" class=\"col_heading level1 col2\" >Recall</th>\n",
       "      <th id=\"T_26e81_level1_col3\" class=\"col_heading level1 col3\" >F1-Score</th>\n",
       "      <th id=\"T_26e81_level1_col4\" class=\"col_heading level1 col4\" >Precision</th>\n",
       "      <th id=\"T_26e81_level1_col5\" class=\"col_heading level1 col5\" >Recall</th>\n",
       "      <th id=\"T_26e81_level1_col6\" class=\"col_heading level1 col6\" >F1-Score</th>\n",
       "      <th id=\"T_26e81_level1_col7\" class=\"col_heading level1 col7\" >Precision</th>\n",
       "      <th id=\"T_26e81_level1_col8\" class=\"col_heading level1 col8\" >Recall</th>\n",
       "      <th id=\"T_26e81_level1_col9\" class=\"col_heading level1 col9\" >F1-Score</th>\n",
       "      <th id=\"T_26e81_level1_col10\" class=\"col_heading level1 col10\" >Precision</th>\n",
       "      <th id=\"T_26e81_level1_col11\" class=\"col_heading level1 col11\" >Recall</th>\n",
       "      <th id=\"T_26e81_level1_col12\" class=\"col_heading level1 col12\" >F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_26e81_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_26e81_row0_col0\" class=\"data row0 col0\" >2984.00</td>\n",
       "      <td id=\"T_26e81_row0_col1\" class=\"data row0 col1\" >0.80</td>\n",
       "      <td id=\"T_26e81_row0_col2\" class=\"data row0 col2\" >0.83</td>\n",
       "      <td id=\"T_26e81_row0_col3\" class=\"data row0 col3\" >0.81</td>\n",
       "      <td id=\"T_26e81_row0_col4\" class=\"data row0 col4\" >0.81</td>\n",
       "      <td id=\"T_26e81_row0_col5\" class=\"data row0 col5\" >0.82</td>\n",
       "      <td id=\"T_26e81_row0_col6\" class=\"data row0 col6\" >0.82</td>\n",
       "      <td id=\"T_26e81_row0_col7\" class=\"data row0 col7\" >0.45</td>\n",
       "      <td id=\"T_26e81_row0_col8\" class=\"data row0 col8\" >0.56</td>\n",
       "      <td id=\"T_26e81_row0_col9\" class=\"data row0 col9\" >0.50</td>\n",
       "      <td id=\"T_26e81_row0_col10\" class=\"data row0 col10\" >0.85</td>\n",
       "      <td id=\"T_26e81_row0_col11\" class=\"data row0 col11\" >0.90</td>\n",
       "      <td id=\"T_26e81_row0_col12\" class=\"data row0 col12\" >0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26e81_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_26e81_row1_col0\" class=\"data row1 col0\" >506.00</td>\n",
       "      <td id=\"T_26e81_row1_col1\" class=\"data row1 col1\" >0.48</td>\n",
       "      <td id=\"T_26e81_row1_col2\" class=\"data row1 col2\" >0.44</td>\n",
       "      <td id=\"T_26e81_row1_col3\" class=\"data row1 col3\" >0.46</td>\n",
       "      <td id=\"T_26e81_row1_col4\" class=\"data row1 col4\" >0.51</td>\n",
       "      <td id=\"T_26e81_row1_col5\" class=\"data row1 col5\" >0.53</td>\n",
       "      <td id=\"T_26e81_row1_col6\" class=\"data row1 col6\" >0.52</td>\n",
       "      <td id=\"T_26e81_row1_col7\" class=\"data row1 col7\" >0.00</td>\n",
       "      <td id=\"T_26e81_row1_col8\" class=\"data row1 col8\" >0.00</td>\n",
       "      <td id=\"T_26e81_row1_col9\" class=\"data row1 col9\" >0.00</td>\n",
       "      <td id=\"T_26e81_row1_col10\" class=\"data row1 col10\" >0.67</td>\n",
       "      <td id=\"T_26e81_row1_col11\" class=\"data row1 col11\" >0.73</td>\n",
       "      <td id=\"T_26e81_row1_col12\" class=\"data row1 col12\" >0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26e81_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_26e81_row2_col0\" class=\"data row2 col0\" >203.00</td>\n",
       "      <td id=\"T_26e81_row2_col1\" class=\"data row2 col1\" >0.32</td>\n",
       "      <td id=\"T_26e81_row2_col2\" class=\"data row2 col2\" >0.20</td>\n",
       "      <td id=\"T_26e81_row2_col3\" class=\"data row2 col3\" >0.24</td>\n",
       "      <td id=\"T_26e81_row2_col4\" class=\"data row2 col4\" >0.00</td>\n",
       "      <td id=\"T_26e81_row2_col5\" class=\"data row2 col5\" >0.00</td>\n",
       "      <td id=\"T_26e81_row2_col6\" class=\"data row2 col6\" >0.00</td>\n",
       "      <td id=\"T_26e81_row2_col7\" class=\"data row2 col7\" >0.00</td>\n",
       "      <td id=\"T_26e81_row2_col8\" class=\"data row2 col8\" >0.00</td>\n",
       "      <td id=\"T_26e81_row2_col9\" class=\"data row2 col9\" >0.00</td>\n",
       "      <td id=\"T_26e81_row2_col10\" class=\"data row2 col10\" >0.65</td>\n",
       "      <td id=\"T_26e81_row2_col11\" class=\"data row2 col11\" >0.45</td>\n",
       "      <td id=\"T_26e81_row2_col12\" class=\"data row2 col12\" >0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26e81_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_26e81_row3_col0\" class=\"data row3 col0\" >389.00</td>\n",
       "      <td id=\"T_26e81_row3_col1\" class=\"data row3 col1\" >0.57</td>\n",
       "      <td id=\"T_26e81_row3_col2\" class=\"data row3 col2\" >0.43</td>\n",
       "      <td id=\"T_26e81_row3_col3\" class=\"data row3 col3\" >0.49</td>\n",
       "      <td id=\"T_26e81_row3_col4\" class=\"data row3 col4\" >0.65</td>\n",
       "      <td id=\"T_26e81_row3_col5\" class=\"data row3 col5\" >0.33</td>\n",
       "      <td id=\"T_26e81_row3_col6\" class=\"data row3 col6\" >0.44</td>\n",
       "      <td id=\"T_26e81_row3_col7\" class=\"data row3 col7\" >0.00</td>\n",
       "      <td id=\"T_26e81_row3_col8\" class=\"data row3 col8\" >0.00</td>\n",
       "      <td id=\"T_26e81_row3_col9\" class=\"data row3 col9\" >0.00</td>\n",
       "      <td id=\"T_26e81_row3_col10\" class=\"data row3 col10\" >0.71</td>\n",
       "      <td id=\"T_26e81_row3_col11\" class=\"data row3 col11\" >0.62</td>\n",
       "      <td id=\"T_26e81_row3_col12\" class=\"data row3 col12\" >0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26e81_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_26e81_row4_col0\" class=\"data row4 col0\" >2958.00</td>\n",
       "      <td id=\"T_26e81_row4_col1\" class=\"data row4 col1\" >0.82</td>\n",
       "      <td id=\"T_26e81_row4_col2\" class=\"data row4 col2\" >0.85</td>\n",
       "      <td id=\"T_26e81_row4_col3\" class=\"data row4 col3\" >0.83</td>\n",
       "      <td id=\"T_26e81_row4_col4\" class=\"data row4 col4\" >0.79</td>\n",
       "      <td id=\"T_26e81_row4_col5\" class=\"data row4 col5\" >0.88</td>\n",
       "      <td id=\"T_26e81_row4_col6\" class=\"data row4 col6\" >0.83</td>\n",
       "      <td id=\"T_26e81_row4_col7\" class=\"data row4 col7\" >0.45</td>\n",
       "      <td id=\"T_26e81_row4_col8\" class=\"data row4 col8\" >0.51</td>\n",
       "      <td id=\"T_26e81_row4_col9\" class=\"data row4 col9\" >0.48</td>\n",
       "      <td id=\"T_26e81_row4_col10\" class=\"data row4 col10\" >0.91</td>\n",
       "      <td id=\"T_26e81_row4_col11\" class=\"data row4 col11\" >0.88</td>\n",
       "      <td id=\"T_26e81_row4_col12\" class=\"data row4 col12\" >0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26e81_level0_row5\" class=\"row_heading level0 row5\" >Average</th>\n",
       "      <td id=\"T_26e81_row5_col0\" class=\"data row5 col0\" >nan</td>\n",
       "      <td id=\"T_26e81_row5_col1\" class=\"data row5 col1\" >0.60</td>\n",
       "      <td id=\"T_26e81_row5_col2\" class=\"data row5 col2\" >0.55</td>\n",
       "      <td id=\"T_26e81_row5_col3\" class=\"data row5 col3\" >0.57</td>\n",
       "      <td id=\"T_26e81_row5_col4\" class=\"data row5 col4\" >0.55</td>\n",
       "      <td id=\"T_26e81_row5_col5\" class=\"data row5 col5\" >0.51</td>\n",
       "      <td id=\"T_26e81_row5_col6\" class=\"data row5 col6\" >0.52</td>\n",
       "      <td id=\"T_26e81_row5_col7\" class=\"data row5 col7\" >0.18</td>\n",
       "      <td id=\"T_26e81_row5_col8\" class=\"data row5 col8\" >0.22</td>\n",
       "      <td id=\"T_26e81_row5_col9\" class=\"data row5 col9\" >0.20</td>\n",
       "      <td id=\"T_26e81_row5_col10\" class=\"data row5 col10\" >0.76</td>\n",
       "      <td id=\"T_26e81_row5_col11\" class=\"data row5 col11\" >0.72</td>\n",
       "      <td id=\"T_26e81_row5_col12\" class=\"data row5 col12\" >0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26e81_level0_row6\" class=\"row_heading level0 row6\" >Weighted Average</th>\n",
       "      <td id=\"T_26e81_row6_col0\" class=\"data row6 col0\" >nan</td>\n",
       "      <td id=\"T_26e81_row6_col1\" class=\"data row6 col1\" >0.76</td>\n",
       "      <td id=\"T_26e81_row6_col2\" class=\"data row6 col2\" >0.77</td>\n",
       "      <td id=\"T_26e81_row6_col3\" class=\"data row6 col3\" >0.76</td>\n",
       "      <td id=\"T_26e81_row6_col4\" class=\"data row6 col4\" >0.75</td>\n",
       "      <td id=\"T_26e81_row6_col5\" class=\"data row6 col5\" >0.77</td>\n",
       "      <td id=\"T_26e81_row6_col6\" class=\"data row6 col6\" >0.76</td>\n",
       "      <td id=\"T_26e81_row6_col7\" class=\"data row6 col7\" >0.38</td>\n",
       "      <td id=\"T_26e81_row6_col8\" class=\"data row6 col8\" >0.45</td>\n",
       "      <td id=\"T_26e81_row6_col9\" class=\"data row6 col9\" >0.42</td>\n",
       "      <td id=\"T_26e81_row6_col10\" class=\"data row6 col10\" >0.85</td>\n",
       "      <td id=\"T_26e81_row6_col11\" class=\"data row6 col11\" >0.85</td>\n",
       "      <td id=\"T_26e81_row6_col12\" class=\"data row6 col12\" >0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e6c146e350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build & display the table using your FILES_DIR\n",
    "comparison_df = evalkit.build_best_models_comparison(FILES_DIR, headers=\"simplified\")\n",
    "evalkit.display_comparison(comparison_df, center_headers=True)\n",
    "\n",
    "# Save the comparison DataFrame to CSV\n",
    "(comparison_df.to_csv(FILES_DIR / \"reports\" / \"best_models_per_class_comparison.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
